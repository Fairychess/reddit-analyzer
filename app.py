import streamlit as st
import json
import os
from datetime import datetime, timedelta
import pandas as pd
from reddit_crawler import RedditCrawler
from data_analyzer import DataAnalyzer
from sentiment_analyzer import SentimentAnalyzer
from topic_analyzer import TopicAnalyzer
from visualizer import DataVisualizer

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Redditå“ç‰Œäº§å“åˆ†æå™¨",
    page_icon="ğŸ“Š",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ“Š Redditå“ç‰Œäº§å“åˆ†æå™¨")
st.markdown("### æ— éœ€APIå‡­è¯ï¼Œä¸€é”®åˆ†æRedditèˆ†æƒ…")

# ä¾§è¾¹æ  - é…ç½®å‚æ•°
st.sidebar.header("âš™ï¸ é…ç½®å‚æ•°")

# å“ç‰Œå’Œäº§å“
brand = st.sidebar.text_input("å“ç‰Œåç§°", value="Apple", help="ä¾‹å¦‚: Apple, Samsung, Sony")
product = st.sidebar.text_input("äº§å“åç§°", value="iPhone 15", help="ä¾‹å¦‚: iPhone 15, Galaxy S24")

# æ—¶é—´èŒƒå›´
st.sidebar.subheader("æ—¶é—´èŒƒå›´")
col1, col2 = st.sidebar.columns(2)

# é»˜è®¤æœ€è¿‘3ä¸ªæœˆ
default_end = datetime.now()
default_start = default_end - timedelta(days=90)

start_date = col1.date_input("å¼€å§‹æ—¥æœŸ", value=default_start)
end_date = col2.date_input("ç»“æŸæ—¥æœŸ", value=default_end)

# Subreddité€‰æ‹©
st.sidebar.subheader("Subreddit")
subreddit_option = st.sidebar.radio(
    "é€‰æ‹©æ–¹å¼",
    ["æ¨èç¤¾åŒº", "è‡ªå®šä¹‰"]
)

if subreddit_option == "æ¨èç¤¾åŒº":
    # é¢„è®¾çš„å¸¸ç”¨subredditç»„åˆ
    preset = st.sidebar.selectbox(
        "é€‰æ‹©é¢„è®¾",
        [
            "ç§‘æŠ€äº§å“ (technology, gadgets)",
            "æ‰‹æœº (smartphone, android, iphone)",
            "æ¸¸æˆ (gaming, PS5, xbox)",
            "ç¬”è®°æœ¬ç”µè„‘ (laptops, thinkpad, macbook)",
            "æ‰€æœ‰ç¤¾åŒº (all)"
        ]
    )

    preset_map = {
        "ç§‘æŠ€äº§å“ (technology, gadgets)": ["technology", "gadgets", "tech"],
        "æ‰‹æœº (smartphone, android, iphone)": ["smartphone", "android", "iphone", "samsung"],
        "æ¸¸æˆ (gaming, PS5, xbox)": ["gaming", "PS5", "xbox", "playstation"],
        "ç¬”è®°æœ¬ç”µè„‘ (laptops, thinkpad, macbook)": ["laptops", "thinkpad", "macbook", "dell"],
        "æ‰€æœ‰ç¤¾åŒº (all)": ["all"]
    }

    subreddits = preset_map[preset]
else:
    subreddit_input = st.sidebar.text_input(
        "è¾“å…¥Subreddit",
        value="technology, apple, iphone",
        help="ç”¨é€—å·åˆ†éš”å¤šä¸ªsubreddit"
    )
    subreddits = [s.strip() for s in subreddit_input.split(",") if s.strip()]

# çˆ¬å–æ•°é‡é™åˆ¶
limit = st.sidebar.slider("æ¯ä¸ªSubredditçˆ¬å–æ•°é‡", min_value=50, max_value=500, value=200, step=50)

# å¼€å§‹åˆ†ææŒ‰é’®
st.sidebar.markdown("---")
start_analysis = st.sidebar.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

# ä¸»ç•Œé¢
if not start_analysis:
    # æ˜¾ç¤ºè¯´æ˜
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é…ç½®å‚æ•°ï¼Œç„¶åç‚¹å‡»ã€Œå¼€å§‹åˆ†æã€æŒ‰é’®")

    st.markdown("### åŠŸèƒ½ç‰¹æ€§")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("#### ğŸ“ˆ æ•°æ®ç»Ÿè®¡")
        st.markdown("""
        - æ€»å£°é‡ç»Ÿè®¡
        - å¸–å­å’Œè¯„è®ºæ•°é‡
        - æ¶‰åŠç”¨æˆ·æ•°
        - Subredditåˆ†å¸ƒ
        """)

    with col2:
        st.markdown("#### ğŸ˜Š æƒ…æ„Ÿåˆ†æ")
        st.markdown("""
        - æ­£é¢/ä¸­æ€§/è´Ÿé¢å æ¯”
        - æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ
        - å„ç¤¾åŒºæƒ…æ„Ÿå¯¹æ¯”
        - æœ€æ­£/è´Ÿé¢å†…å®¹
        """)

    with col3:
        st.markdown("#### ğŸ” è¯é¢˜åˆ†æ")
        st.markdown("""
        - å…³é”®è¯æå–
        - è¯é¢˜èšç±»
        - è¯äº‘ç”Ÿæˆ
        - çƒ­é—¨è®¨è®º
        """)

    st.markdown("---")
    st.markdown("### ä½¿ç”¨æç¤º")
    st.markdown("""
    1. **æ—¶é—´èŒƒå›´**: å»ºè®®é€‰æ‹©1-3ä¸ªæœˆï¼ŒèŒƒå›´å¤ªå¤§ä¼šå½±å“é€Ÿåº¦
    2. **Subreddit**: å¯ä»¥é€‰æ‹©ç‰¹å®šç¤¾åŒºè·å¾—æ›´ç²¾å‡†çš„ç»“æœ
    3. **çˆ¬å–æ•°é‡**: å»ºè®®è®¾ç½®100-300ï¼Œæ•°é‡è¶Šå¤§æ—¶é—´è¶Šé•¿
    4. **åˆ†ææ—¶é—´**: æ ¹æ®æ•°æ®é‡ï¼Œé€šå¸¸éœ€è¦3-10åˆ†é’Ÿ
    """)

else:
    # å¼€å§‹åˆ†æ
    try:
        # åˆ›å»ºé…ç½®
        config = {
            "search": {
                "brand": brand,
                "product": product,
                "start_date": start_date.strftime("%d/%m/%Y"),
                "end_date": end_date.strftime("%d/%m/%Y"),
                "subreddits": subreddits,
                "limit": limit
            }
        }

        # ä¿å­˜é…ç½®
        with open('config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        st.success(f"æ­£åœ¨åˆ†æ: {brand} {product}")
        st.info(f"æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
        st.info(f"æœç´¢ç¤¾åŒº: {', '.join(subreddits)}")

        # è¿›åº¦æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        # æ­¥éª¤1: çˆ¬å–æ•°æ®
        status_text.text("æ­¥éª¤ 1/6: æ­£åœ¨çˆ¬å–Redditæ•°æ®...")
        progress_bar.progress(10)

        crawler = RedditCrawler('config.json')
        posts_data, comments_data = crawler.search_posts()

        if not posts_data and not comments_data:
            st.error("âŒ æœªæ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œè¯·å°è¯•ï¼š")
            st.markdown("""
            - æ‰©å¤§æ—¶é—´èŒƒå›´
            - ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
            - é€‰æ‹©æ›´å¤šçš„Subreddit
            """)
            st.stop()

        progress_bar.progress(25)

        # æ­¥éª¤2: æ•°æ®ç»Ÿè®¡
        status_text.text("æ­¥éª¤ 2/6: æ­£åœ¨è¿›è¡Œæ•°æ®ç»Ÿè®¡...")
        analyzer = DataAnalyzer(posts_data, comments_data)
        basic_stats = analyzer.calculate_basic_stats()
        subreddit_distribution = analyzer.get_subreddit_distribution()
        time_distribution = analyzer.get_time_distribution()
        engagement_stats = analyzer.get_engagement_stats()
        top_posts = analyzer.get_top_posts()

        progress_bar.progress(40)

        # æ­¥éª¤3: æƒ…æ„Ÿåˆ†æ
        status_text.text("æ­¥éª¤ 3/6: æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
        sentiment_analyzer = SentimentAnalyzer(posts_data, comments_data)
        posts_with_sentiment = sentiment_analyzer.analyze_posts()
        comments_with_sentiment = sentiment_analyzer.analyze_comments()
        sentiment_distribution = sentiment_analyzer.get_sentiment_distribution()
        sentiment_by_subreddit = sentiment_analyzer.get_sentiment_by_subreddit()

        progress_bar.progress(60)

        # æ­¥éª¤4: è¯é¢˜åˆ†æ
        status_text.text("æ­¥éª¤ 4/6: æ­£åœ¨è¿›è¡Œè¯é¢˜åˆ†æ...")
        topic_analyzer = TopicAnalyzer(posts_with_sentiment, comments_with_sentiment)
        keywords = topic_analyzer.extract_keywords(top_n=30)
        topics = topic_analyzer.extract_topic_clusters()

        progress_bar.progress(75)

        # æ­¥éª¤5: ç”Ÿæˆå¯è§†åŒ–
        status_text.text("æ­¥éª¤ 5/6: æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        visualizer = DataVisualizer(output_dir='output')
        visualizer.plot_sentiment_distribution(sentiment_distribution)
        visualizer.plot_subreddit_distribution(subreddit_distribution)
        visualizer.plot_time_series(time_distribution)
        if topics:
            visualizer.plot_topic_distribution(topics)
        if sentiment_by_subreddit:
            visualizer.plot_sentiment_by_subreddit(sentiment_by_subreddit)
        topic_analyzer.generate_wordcloud()

        progress_bar.progress(90)

        # æ­¥éª¤6: å¯¼å‡ºæ•°æ®
        status_text.text("æ­¥éª¤ 6/6: æ­£åœ¨å¯¼å‡ºç»“æœ...")
        visualizer.export_to_csv(posts_with_sentiment, comments_with_sentiment)

        progress_bar.progress(100)
        status_text.text("âœ… åˆ†æå®Œæˆï¼")

        # æ˜¾ç¤ºç»“æœ
        st.success("ğŸ‰ åˆ†æå®Œæˆï¼")

        # Tabå¸ƒå±€
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š åŸºç¡€ç»Ÿè®¡", "ğŸ˜Š æƒ…æ„Ÿåˆ†æ", "ğŸ” è¯é¢˜åˆ†æ", "ğŸ“¥ æ•°æ®ä¸‹è½½"])

        with tab1:
            st.header("åŸºç¡€ç»Ÿè®¡")

            # å…³é”®æŒ‡æ ‡
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("æ€»å£°é‡", f"{basic_stats['total_volume']:,}")
            col2.metric("å¸–å­æ•°", f"{basic_stats['post_count']:,}")
            col3.metric("è¯„è®ºæ•°", f"{basic_stats['comment_count']:,}")
            col4.metric("æ¶‰åŠç”¨æˆ·", f"{basic_stats['unique_users']:,}")

            st.markdown("---")

            # å›¾è¡¨
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("Subredditåˆ†å¸ƒ")
                if os.path.exists('output/subreddit_distribution.png'):
                    st.image('output/subreddit_distribution.png')

            with col2:
                st.subheader("æ—¶é—´åºåˆ—è¶‹åŠ¿")
                if os.path.exists('output/time_series.png'):
                    st.image('output/time_series.png')

            # çƒ­é—¨å¸–å­
            st.subheader("ğŸ”¥ çƒ­é—¨å¸–å­ Top 10")
            if top_posts:
                for i, post in enumerate(top_posts[:10], 1):
                    with st.expander(f"{i}. {post['title'][:80]}..."):
                        st.write(f"**ä½œè€…**: u/{post['author']}")
                        st.write(f"**ç¤¾åŒº**: r/{post['subreddit']}")
                        st.write(f"**å¾—åˆ†**: {post['score']} | **è¯„è®ºæ•°**: {post['num_comments']}")
                        st.write(f"**å‘å¸ƒæ—¶é—´**: {post['created_date']}")
                        st.markdown(f"[æŸ¥çœ‹åŸå¸–]({post['permalink']})")

        with tab2:
            st.header("æƒ…æ„Ÿåˆ†æ")

            # æƒ…æ„Ÿåˆ†å¸ƒæ¦‚è§ˆ
            col1, col2, col3 = st.columns(3)
            col1.metric(
                "ğŸ˜Š æ­£é¢",
                f"{sentiment_distribution['positive']['percentage']}%",
                f"{sentiment_distribution['positive']['count']} æ¡"
            )
            col2.metric(
                "ğŸ˜ ä¸­æ€§",
                f"{sentiment_distribution['neutral']['percentage']}%",
                f"{sentiment_distribution['neutral']['count']} æ¡"
            )
            col3.metric(
                "ğŸ˜ è´Ÿé¢",
                f"{sentiment_distribution['negative']['percentage']}%",
                f"{sentiment_distribution['negative']['count']} æ¡"
            )

            st.markdown("---")

            # æƒ…æ„Ÿå›¾è¡¨
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("æ•´ä½“æƒ…æ„Ÿåˆ†å¸ƒ")
                if os.path.exists('output/sentiment_distribution.png'):
                    st.image('output/sentiment_distribution.png')

            with col2:
                st.subheader("å„Subredditæƒ…æ„Ÿå¯¹æ¯”")
                if os.path.exists('output/sentiment_by_subreddit.png'):
                    st.image('output/sentiment_by_subreddit.png')

        with tab3:
            st.header("è¯é¢˜åˆ†æ")

            # è¯äº‘
            st.subheader("è¯äº‘å›¾")
            if os.path.exists('output/wordcloud.png'):
                st.image('output/wordcloud.png', use_container_width=True)

            st.markdown("---")

            col1, col2 = st.columns(2)

            with col1:
                # å…³é”®è¯
                st.subheader("ğŸ”‘ é«˜é¢‘å…³é”®è¯ Top 20")
                if keywords:
                    keywords_df = pd.DataFrame(
                        list(keywords.items())[:20],
                        columns=['å…³é”®è¯', 'å‡ºç°æ¬¡æ•°']
                    )
                    st.dataframe(keywords_df, use_container_width=True, hide_index=True)

            with col2:
                # è¯é¢˜åˆ†å¸ƒ
                st.subheader("ğŸ“Š è¯é¢˜åˆ†å¸ƒ")
                if os.path.exists('output/topic_distribution.png'):
                    st.image('output/topic_distribution.png')

        with tab4:
            st.header("æ•°æ®ä¸‹è½½")

            st.markdown("### ğŸ“¥ å¯¼å‡ºæ–‡ä»¶")

            # CSVæ–‡ä»¶ä¸‹è½½
            col1, col2 = st.columns(2)

            with col1:
                if os.path.exists('output/reddit_data_posts.csv'):
                    with open('output/reddit_data_posts.csv', 'rb') as f:
                        st.download_button(
                            label="ğŸ“„ ä¸‹è½½å¸–å­æ•°æ® (CSV)",
                            data=f,
                            file_name=f"{brand}_{product}_posts.csv",
                            mime="text/csv"
                        )

            with col2:
                if os.path.exists('output/reddit_data_comments.csv'):
                    with open('output/reddit_data_comments.csv', 'rb') as f:
                        st.download_button(
                            label="ğŸ’¬ ä¸‹è½½è¯„è®ºæ•°æ® (CSV)",
                            data=f,
                            file_name=f"{brand}_{product}_comments.csv",
                            mime="text/csv"
                        )

            # æŠ¥å‘Šä¸‹è½½
            st.markdown("### ğŸ“Š åˆ†ææŠ¥å‘Š")

            col1, col2 = st.columns(2)

            with col1:
                if os.path.exists('output/analysis_report.json'):
                    with open('output/analysis_report.json', 'rb') as f:
                        st.download_button(
                            label="ğŸ“‹ ä¸‹è½½å®Œæ•´æŠ¥å‘Š (JSON)",
                            data=f,
                            file_name=f"{brand}_{product}_report.json",
                            mime="application/json"
                        )

            with col2:
                if os.path.exists('output/analysis_report.txt'):
                    with open('output/analysis_report.txt', 'rb') as f:
                        st.download_button(
                            label="ğŸ“ ä¸‹è½½ç®€è¦æŠ¥å‘Š (TXT)",
                            data=f,
                            file_name=f"{brand}_{product}_report.txt",
                            mime="text/plain"
                        )

            # å›¾è¡¨ä¸‹è½½è¯´æ˜
            st.markdown("### ğŸ–¼ï¸ å›¾è¡¨æ–‡ä»¶")
            st.info("æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åœ¨ `output/` ç›®å½•ï¼ŒåŒ…æ‹¬ï¼š\n- sentiment_distribution.png\n- subreddit_distribution.png\n- time_series.png\n- topic_distribution.png\n- sentiment_by_subreddit.png\n- wordcloud.png")

    except Exception as e:
        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        st.code(traceback.format_exc())

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>Redditå“ç‰Œäº§å“åˆ†æå™¨ | æ— éœ€APIå‡­è¯ï¼Œç®€å•æ˜“ç”¨</p>
    <p>åŸºäºPython + Streamlitæ„å»º</p>
</div>
""", unsafe_allow_html=True)
